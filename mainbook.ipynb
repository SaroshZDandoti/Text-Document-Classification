{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hello, \r\n",
    "### Welcome to the Document Classification Notebook.\r\n",
    "\r\n",
    "In this project we will classify text document files from labelled folders and then predict for a single document.\r\n",
    "I have made this project as simple as possible to explain.\r\n",
    " \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "Firstly lets import all our required packages...\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "import os\r\n",
    "import spacy\r\n",
    "import codecs\r\n",
    "import pickle as pkl\r\n",
    "import pandas as pd\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\r\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\r\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.model_selection import KFold, cross_val_score\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Labelling and TfidfVectorizer\r\n",
    "\r\n",
    "Here firstly we will take the folder in which the labelled folders are located and run LabelEncoder()\r\n",
    "\r\n",
    "<h3> LabelEncoder() </h3>\r\n",
    "- Basically what you need to understand here is this function converts labels into numeric form  so  \r\n",
    "    our ML models can work properly with them. This is an important Preprocessing Step.\r\n",
    "  \r\n",
    "\r\n",
    "<h3> TfidfVectorizer() </h3>  \r\n",
    " - Term Frequency Inverse Document Frequency.\r\n",
    "             In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with   \r\n",
    "            most frequent words. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.    \r\n",
    " \r\n",
    "     \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Label Encoding\r\n",
    "TRAIN_DATA_DIR = \"data\"\r\n",
    "data_dir = TRAIN_DATA_DIR \r\n",
    "\r\n",
    "def set_labels():\r\n",
    "    categories = [f.lower() for f in os.listdir(data_dir)\r\n",
    "                    if os.path.isdir(os.path.join(data_dir, f))]\r\n",
    "    le = LabelEncoder()\r\n",
    "    le.fit(categories)\r\n",
    "    print( \"Categories (Labels) : \", categories)\r\n",
    "    return le\r\n",
    "\r\n",
    "label_encoder = set_labels()\r\n",
    "\r\n",
    "# TfidfVectorizer\r\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, decode_error='replace', min_df=2)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Categories (Labels) :  ['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Processing\r\n",
    "We will now preprocess the text data.  \r\n",
    "We have two methods inside a DataProcessing Class.  \r\n",
    "\r\n",
    "process_data() - In this function we are taking the text files and the labels and storing them in a dataframe.  \r\n",
    "We are applying tfidfVectorizer on our dataset.\r\n",
    "\r\n",
    "process_test_data() - same for testing for sample input"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class DataProcessing:\r\n",
    "\r\n",
    "    def process_data(self):\r\n",
    "        print(\" Processing data \")\r\n",
    "        dataset = pd.DataFrame()\r\n",
    "        categories = label_encoder.classes_\r\n",
    "        # Loop through the Directory , loop through the files, process them like removing spaces, making them lowercase \r\n",
    "        for cat in categories: # folders\r\n",
    "            nlp = spacy.load('en_core_web_sm')\r\n",
    "            for f in os.listdir(os.path.join(TRAIN_DATA_DIR, cat)): # files in a folder\r\n",
    "                try:\r\n",
    "                    f_read = open(os.path.join(TRAIN_DATA_DIR, cat, f), 'r') \r\n",
    "                    \r\n",
    "                    doc = \" \".join( [ line.strip().lower() for line in f_read ] )\r\n",
    "                    doc_vec = \" \".join([ word.lemma_ for word in nlp(doc)\r\n",
    "                                         if not (word.is_space or word.is_stop or word.is_punct or ('\\n' in word.text) ) ] )\r\n",
    "                \r\n",
    "                    label = pd.Series(label_encoder.transform([cat]), dtype='Int64')\r\n",
    "        \r\n",
    "                    if len(doc_vec.strip()) > 0:\r\n",
    "                        dataset = dataset.append({'sentences': doc_vec, 'labels': label[0]}, ignore_index=True)\r\n",
    "              \r\n",
    "                except ValueError as e:\r\n",
    "                    print(f)\r\n",
    "                    print(e)\r\n",
    "\r\n",
    "        #Creating the DataFrame and applying tfidf to the texts.    \r\n",
    "        vectors = pd.DataFrame(tfidf_vectorizer.fit_transform(dataset['sentences']).toarray())\r\n",
    "        dataset = dataset.join(vectors)\r\n",
    "        #Splitting the Dataset into Train and Test\r\n",
    "        data_train, data_test = train_test_split(dataset, test_size=0.3, random_state=1234)\r\n",
    "        #print(data_train.head())\r\n",
    "        #print(data_test.head())\r\n",
    "        return data_train, data_test\r\n",
    "\r\n",
    "    # process test phrases on new data\r\n",
    "    def process_test_data(self, test_data):\r\n",
    "        test_files = []\r\n",
    "        if os.path.isdir(test_data):\r\n",
    "            test_files += [os.path.join(test_data, f) for f in os.listdir(test_data)]\r\n",
    "        else:\r\n",
    "            test_files.append(test_data)\r\n",
    "        \r\n",
    "        test_dataset = pd.DataFrame()\r\n",
    "        nlp = spacy.load('en_core_web_sm')\r\n",
    "        \r\n",
    "        for f in test_files:\r\n",
    "            doc = \" \".join([line.lower() for line in open(f, 'r', encoding='utf-8')])\r\n",
    "            doc_vec = \" \".join([word.lemma_ for word in nlp(doc)\r\n",
    "                                if not (word.is_space or word.is_stop or word.is_punct or ('\\n' in word.text))])\r\n",
    "            \r\n",
    "            test_dataset = test_dataset.append({'sentences': doc_vec, 'file_names': f}, ignore_index=True)\r\n",
    "        \r\n",
    "        return test_dataset.join(pd.DataFrame(tfidf_vectorizer.transform(test_dataset['sentences']).toarray()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now lets call our Class\r\n",
    " and call the process_data() method on the dataset\r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "proc = DataProcessing()\r\n",
    "encoder = label_encoder\r\n",
    "vectorizer = tfidf_vectorizer\r\n",
    "data_train, data_test = proc.process_data()  \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Processing data \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_train.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>howl help boost japan cinema japan box office ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116785</td>\n",
       "      <td>0.422422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>china net cafe culture crackdown chinese autho...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.029559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>french suitor hold lse meeting european stock ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  labels         0  \\\n",
       "555   howl help boost japan cinema japan box office ...     1.0  0.000000   \n",
       "1825  china net cafe culture crackdown chinese autho...     4.0  0.029559   \n",
       "263   french suitor hold lse meeting european stock ...     0.0  0.000000   \n",
       "\n",
       "        1         2    3         4         5    6    7  ...  1990  1991  1992  \\\n",
       "555   0.0  0.000000  0.0  0.000000  0.060623  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1825  0.0  0.040124  0.0  0.076752  0.000000  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "263   0.0  0.000000  0.0  0.000000  0.000000  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "          1993      1994      1995      1996  1997  1998  1999  \n",
       "555   0.116785  0.422422  0.000000  0.000000   0.0   0.0   0.0  \n",
       "1825  0.000000  0.000000  0.000000  0.082146   0.0   0.0   0.0  \n",
       "263   0.000000  0.000000  0.038002  0.000000   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 2002 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data_test.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>lord scarman 93 die peacefully distinguished l...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>high definition dvds humble home video dvd hol...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>lesotho textile worker lose job foreign own te...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentences  labels         0  \\\n",
       "1014  lord scarman 93 die peacefully distinguished l...     2.0  0.000000   \n",
       "2136  high definition dvds humble home video dvd hol...     4.0  0.000000   \n",
       "441   lesotho textile worker lose job foreign own te...     0.0  0.043979   \n",
       "\n",
       "        1    2    3    4    5    6    7  ...  1990  1991  1992      1993  \\\n",
       "1014  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0  0.017538   \n",
       "2136  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0  0.009927   \n",
       "441   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0  0.000000   \n",
       "\n",
       "      1994  1995      1996  1997  1998  1999  \n",
       "1014   0.0   0.0  0.042417   0.0   0.0   0.0  \n",
       "2136   0.0   0.0  0.000000   0.0   0.0   0.0  \n",
       "441    0.0   0.0  0.000000   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 2002 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Generation\r\n",
    "Lets create a class and make our models as methods.\r\n",
    "We will use GridSearchCV for Hyperparameter Tuning.\r\n",
    "\r\n",
    "We have used Naive Bayes, KNN , SVM , RandomForest.\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class ModelGenerator:\r\n",
    "\r\n",
    "    def __init__(self, data_train, data_test=None):\r\n",
    "        self.data_train = data_train\r\n",
    "        self.data_test = data_test\r\n",
    "\r\n",
    "    def naive_bayes(self):\r\n",
    "        model = MNB()\r\n",
    "        params = {}\r\n",
    "        return self.cross_validation(model,params)\r\n",
    "\r\n",
    "    def knn(self): \r\n",
    "        model = KNN()\r\n",
    "        params = {'n_neighbors': [3, 4, 5, 6],\r\n",
    "                  'weights': ['uniform', 'distance'], \r\n",
    "                  'metric': ['euclidean', 'manhattan']}\r\n",
    "        return self.cross_validation(model, params) \r\n",
    "\r\n",
    "    def svm(self):\r\n",
    "        model = SVC()\r\n",
    "        params = {'C': [0.1, 1, 10, 100, 1000],\r\n",
    "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\r\n",
    "                  'kernel': ['rbf']}\r\n",
    "        return self.cross_validation(model, params)\r\n",
    "\r\n",
    "    def random_forest(self):\r\n",
    "        model = RFC() \r\n",
    "        params = {'n_estimators': [100, 200, 400],\r\n",
    "                  'criterion': ['gini', 'entropy'],\r\n",
    "                  'max_features': ['auto', 'sqrt', 'log2']}\r\n",
    "        return self.cross_validation(model, params) \r\n",
    "\r\n",
    "    def cross_validation(self, model, params):\r\n",
    "        cols = self.data_train.columns.difference(['labels', 'sentences'])\r\n",
    "        cv = KFold(n_splits=10, random_state=1, shuffle=True)\r\n",
    "        gcv = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', n_jobs=-1, cv=cv)\r\n",
    "        gcv.fit(self.data_train[cols], self.data_train['labels'])\r\n",
    "        return gcv\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Initialization\r\n",
    "Creating our models now ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# generate and train models \r\n",
    "model_gen = ModelGenerator(data_train) \r\n",
    "nb = model_gen.naive_bayes()\r\n",
    "knn = model_gen.knn()      \r\n",
    "svm = model_gen.svm() \r\n",
    "rf = model_gen.random_forest() \r\n",
    "\r\n",
    "models = {'nb': nb, 'knn': knn, 'svm': svm, 'rf':rf}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving our models and Evaluations\r\n",
    "Save all the models by just calling the save function.  \r\n",
    "We will print a classification report to see the details of the models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "SAVE_MODEL_PATH = \"save_model\"  \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def evaluate_model(cl_model, test_data):\r\n",
    "    print(cl_model.best_estimator_) \r\n",
    "    cols = test_data.columns.difference(['sentences', 'labels'])\r\n",
    "    predictions = list(map(int, cl_model.predict(test_data[cols])))\r\n",
    "    c_report = classification_report(test_data['labels'], predictions)\r\n",
    "    print(c_report)\r\n",
    "\r\n",
    "def save_model(name, model_files):\r\n",
    "    filename = os.path.join(SAVE_MODEL_PATH, name + '.pkl')\r\n",
    "    with open(filename, 'wb') as fh:\r\n",
    "        pkl.dump(model_files, fh)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "print(\"Naive Bayes: \")\r\n",
    "evaluate_model(models['nb'], data_test )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes: \n",
      "MultinomialNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.92      0.95       172\n",
      "         1.0       0.97      0.95      0.96       113\n",
      "         2.0       0.90      0.97      0.93       115\n",
      "         3.0       0.98      0.99      0.99       140\n",
      "         4.0       0.95      0.95      0.95       128\n",
      "\n",
      "    accuracy                           0.96       668\n",
      "   macro avg       0.95      0.96      0.95       668\n",
      "weighted avg       0.96      0.96      0.96       668\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "print(\"KNN: \")\r\n",
    "evaluate_model(models['knn'], data_test )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KNN: \n",
      "KNeighborsClassifier(metric='euclidean', n_neighbors=6, weights='distance')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.86      0.91       172\n",
      "         1.0       0.95      0.89      0.92       113\n",
      "         2.0       0.84      0.97      0.90       115\n",
      "         3.0       0.95      0.99      0.97       140\n",
      "         4.0       0.95      0.94      0.94       128\n",
      "\n",
      "    accuracy                           0.93       668\n",
      "   macro avg       0.93      0.93      0.93       668\n",
      "weighted avg       0.93      0.93      0.93       668\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "print(\"SVM: \")\r\n",
    "evaluate_model(models['svm'], data_test )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM: \n",
      "SVC(C=1, gamma=1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.91      0.94       172\n",
      "         1.0       0.97      0.99      0.98       113\n",
      "         2.0       0.91      0.96      0.93       115\n",
      "         3.0       0.97      0.99      0.98       140\n",
      "         4.0       0.97      0.95      0.96       128\n",
      "\n",
      "    accuracy                           0.96       668\n",
      "   macro avg       0.96      0.96      0.96       668\n",
      "weighted avg       0.96      0.96      0.96       668\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print(\"Random Forest: \")\r\n",
    "evaluate_model(models['rf'], data_test )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest: \n",
      "RandomForestClassifier(max_features='log2', n_estimators=200)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.94      0.95       172\n",
      "         1.0       0.95      0.94      0.95       113\n",
      "         2.0       0.93      0.94      0.94       115\n",
      "         3.0       0.95      0.99      0.97       140\n",
      "         4.0       0.97      0.94      0.95       128\n",
      "\n",
      "    accuracy                           0.95       668\n",
      "   macro avg       0.95      0.95      0.95       668\n",
      "weighted avg       0.95      0.95      0.95       668\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# save models - save to pickle file : model object & processor object\r\n",
    "for name,model in models.items():\r\n",
    "    save_model(name , { 'model': model, 'processor': proc })\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the data\r\n",
    "for samples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "TEST_DATA_DIR = \"test_data\"\r\n",
    "   \r\n",
    "\r\n",
    "# load a save model and predict from given test files\r\n",
    "def run_save_model(model_file_path):\r\n",
    "    model_files = pkl.load(open(model_file_path, 'rb'))\r\n",
    "    processor = model_files['processor']\r\n",
    "    model = model_files['model']\r\n",
    "    \r\n",
    "    test_data = processor.process_test_data(TEST_DATA_DIR) # get file names\r\n",
    "    cols = test_data.columns.difference(['sentences', 'file_names']) #take all colms except 2\r\n",
    "    predict = list(map(int, model.predict(test_data[cols]))) \r\n",
    "\r\n",
    "    results = pd.DataFrame({'file_name': test_data['file_names'], \r\n",
    "                        'classification': label_encoder.inverse_transform(predict)}) # inverse transform on encoder to get classes label \r\n",
    "    return results\r\n",
    "\r\n",
    "\r\n",
    "saved_model_path = 'save_model/nb.pkl'\r\n",
    "results = run_save_model(saved_model_path)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results!\r\n",
    "Woohoo we have successfully classified the text files and trained our models with model tuning.\r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data\\008.txt</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data\\business.txt</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data\\entertainment.txt</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data\\sport.txt</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data\\tech.txt</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file_name classification\n",
       "0            test_data\\008.txt  entertainment\n",
       "1       test_data\\business.txt       business\n",
       "2  test_data\\entertainment.txt  entertainment\n",
       "3          test_data\\sport.txt          sport\n",
       "4           test_data\\tech.txt           tech"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "interpreter": {
   "hash": "df611944fbbb4af2246a3b1d09e3a65922eccc828cede16a0e21d7d353519ec8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}